{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import nltk, re, json, string\n",
    "from nltk.util import ngrams\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet \n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.metrics import pairwise_distances\n",
    "import numpy as np  \n",
    "import pandas as pd\n",
    "import string\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "novel_data = []\n",
    "\n",
    "for i in os.listdir('Dataset/'):\n",
    "    with open('Dataset/'+i, encoding=\"utf8\", errors=\"ignore\") as file:\n",
    "        content = file.read().rstrip().replace(\"\\n\", \"\")\n",
    "        \n",
    "        novel_data.append(content)\n",
    "\n",
    "print(len(novel_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function\n",
    "\n",
    "def nltk_pos_tagger(nltk_tag):\n",
    "    if nltk_tag.startswith('J'):\n",
    "        return(wordnet.ADJ)\n",
    "    elif nltk_tag.startswith('V'):\n",
    "        return(wordnet.VERB)\n",
    "    elif nltk_tag.startswith('N'):\n",
    "        return(wordnet.NOUN)\n",
    "    elif nltk_tag.startswith('R'):\n",
    "        return(wordnet.ADV)\n",
    "    else:          \n",
    "        return None\n",
    "\n",
    "def tokenize(doc):\n",
    "    \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    word_token = nltk.word_tokenize(doc)\n",
    "\n",
    "    unigram = list(ngrams(word_token, 1))\n",
    "    unigram = [i[0] for i in unigram]\n",
    "\n",
    "    unigram = [i for i in unigram if i not in stop_words]\n",
    "    unigram = [i for i in unigram if i not in string.punctuation]\n",
    "    unigram = [i for i in unigram if i!= ('â€”' or '``' or \"''\")]\n",
    "    unigram = [i.lower() for i in unigram]\n",
    "    \n",
    "    pos_tag = nltk.pos_tag(unigram)\n",
    "\n",
    "    clean_unigram = []\n",
    "    \n",
    "    for i in pos_tag:\n",
    "        try:\n",
    "            clean_unigram.append(lemmatizer.lemmatize(i[0], nltk_pos_tagger(i[1])))\n",
    "        except:\n",
    "            clean_unigram.append(i[0])\n",
    "       \n",
    "    return(clean_unigram)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_tfidf(docs):\n",
    "    \n",
    "    smoothed_tf_idf = None\n",
    "    \n",
    "    def get_doc_tokens(i):\n",
    "            tokens = tokenize(i)\n",
    "            token_count=nltk.FreqDist(tokens)\n",
    "            return token_count\n",
    "        \n",
    "    docs_tokens={idx:get_doc_tokens(doc) for idx,doc in enumerate(docs)}\n",
    "   \n",
    "    # put words as columns\n",
    "    dtm=pd.DataFrame.from_dict(docs_tokens, orient=\"index\")\n",
    "    dtm = dtm.sort_index(axis = 0)\n",
    "    dtm=dtm.fillna(0)\n",
    "    tf=dtm.values\n",
    "    \n",
    "    # sum of each rows\n",
    "    doc_len=tf.sum(axis=1)\n",
    "    tf=np.divide(tf, doc_len[:,None])\n",
    "\n",
    "    # find freq of each term in all docs\n",
    "    df=np.where(tf>0,1,0)\n",
    "       \n",
    "    idf=np.log(np.divide(len(docs), np.sum(df, axis=0)))+1\n",
    "  \n",
    "    smoothed_idf=np.log(np.divide(len(docs)+1, np.sum(df, axis=0)+1)+1)\n",
    "   \n",
    "    s = tf*idf\n",
    "    tf_idf=normalize(tf*idf)   \n",
    "    smoothed_tf_idf = normalize(tf*smoothed_idf)\n",
    "        \n",
    "    return smoothed_tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_novel = compute_tfidf(novel_data)\n",
    "tf_idf_novel.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(tf_idf_novel)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('./giantNP.csv', header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.cluster import KMeans\n",
    "# import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = KMeans(init=\"random\", n_clusters=5, n_init=10, max_iter=300, random_state=42).fit(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.inertia_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.n_iter_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kmeans_kwargs = {\n",
    "#    ...:     \"init\": \"random\",\n",
    "#    ...:     \"n_init\": 10,\n",
    "#    ...:     \"max_iter\": 300,\n",
    "#    ...:     \"random_state\": 42,\n",
    "#    ...: }\n",
    "#    ...:\n",
    "#    ...: # A list holds the SSE values for each k\n",
    "#    ...: sse = []\n",
    "#    ...: for k in range(1, 15):\n",
    "#    ...:     kmeans = KMeans(n_clusters=k, **kmeans_kwargs)\n",
    "#    ...:     kmeans.fit(df)\n",
    "#    ...:     sse.append(kmeans.inertia_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(range(1, 15), sse)\n",
    "# plt.xticks(range(1, 15))\n",
    "# plt.xlabel(\"Number of Clusters\")\n",
    "# plt.ylabel(\"SSE\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
